{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain: Industrial Safety Support - NLP based ChatBot - Safex Pybot Version 1.0 \n",
    "\n",
    "#### Context : \n",
    "\n",
    "**** Great Learning Company is looking forward to design an automation which can interact with the user, understand the problem      and display the resolution procedure \n",
    "\n",
    "**** The company is looking for a designed chatbot utility which can help the professionals to highlight the safety risk as per      the incident description usign a ML or DL or redirect the request to an actual human support executive if the request is        complex or not in its database.\n",
    "\n",
    "#### Data Description : \n",
    "\n",
    "**** The corpus is attached for the reference. Please enhance/add more data to the corpus using your linguistics skills."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Importing the data and files\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from scipy import stats; from scipy.stats import zscore, norm, randint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:111: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "curses is not supported on this machine (please install/reinstall curses for an optimal experience)\n"
     ]
    }
   ],
   "source": [
    "# import the libraries\n",
    "\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# things we need for Tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "import tflearn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and functions for Question and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "1a10da399a40c2386b7a892db2f725bf2c2a4d91",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('GLSafexInfo.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f5bfc031e0908e044f0bb27ebd1fbf7bac7e66fe",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize data fields for our file\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "d402102e6a2fe4b3d3abf6cf36338878c07402f8",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "X_train = list(training[:,0])\n",
    "y_train = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "d9ea1e5769bdd9c92282a502caa47a16db302ff5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12999  | total loss: \u001b[1m\u001b[32m0.10005\u001b[0m\u001b[0m | time: 0.038s\n",
      "| Adam | epoch: 1000 | loss: 0.10005 - acc: 0.9912 -- iter: 096/101\n",
      "Training Step: 13000  | total loss: \u001b[1m\u001b[32m0.09260\u001b[0m\u001b[0m | time: 0.038s\n",
      "| Adam | epoch: 1000 | loss: 0.09260 - acc: 0.9921 -- iter: 101/101\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Dell\\Desktop\\GreatLearning PGP AIML\\Capstone Project\\Milestone 2\\Apoorv Working\\model_qna.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# reset underlying graph data\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Build neural network\n",
    "net_qna = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "net_qna = tflearn.fully_connected(net_qna, 16)\n",
    "net_qna = tflearn.fully_connected(net_qna, 16)\n",
    "net_qna = tflearn.fully_connected(net_qna, len(y_train[0]), activation='softmax')\n",
    "net_qna = tflearn.regression(net_qna)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model_qna = tflearn.DNN(net_qna, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model_qna.fit(X_train, y_train, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model_qna.save('model_qna.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "f3df3637327eb3b2c2a63dfd2c34b1e8ffa135bb",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'X_train':X_train, 'y_train':y_train}, open( \"qna_training_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "da5da061a3b18179176c491d0938412309823cff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data_qna = pickle.load( open( \"qna_training_data\", \"rb\" ) )\n",
    "words_qna = data_qna['words']\n",
    "classes_qna = data_qna['classes']\n",
    "X_train_qna = data_qna['X_train']\n",
    "y_train_qna = data_qna['y_train']\n",
    "\n",
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('GLSafexInfo.json') as json_data:\n",
    "    intents_qna = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "6dbca05cc97cd1c66896ec00a37bb0f932b02486",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Dell\\Desktop\\GreatLearning PGP AIML\\Capstone Project\\Milestone 2\\Apoorv Working\\model_qna.tflearn\n"
     ]
    }
   ],
   "source": [
    "# load our saved model\n",
    "model_qna.load('./model_qna.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "0c4c7d32c560284a20da8b659075347f8eee17b5",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functions for cleansing the sentenses and to get the bag of words\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stem each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# return bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "4774b1be2f6419c305e36d45c4224452064128d8",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# functions to classify the sentences and for respective responses\n",
    "# create a data structure to hold user context\n",
    "\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model_qna.predict([bow(sentence, words_qna)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes_qna[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def response(sentence, show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents_qna['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # a random response from the intent\n",
    "                    return print('BOT: ', random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot : Historical Data Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset is : (418, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Country</th>\n",
       "      <th>Local</th>\n",
       "      <th>Industry Sector</th>\n",
       "      <th>Accident Level</th>\n",
       "      <th>Potential Accident Level</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Natureofemployee</th>\n",
       "      <th>Critical Risk</th>\n",
       "      <th>Description</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>Weekday</th>\n",
       "      <th>Season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016-01-01</td>\n",
       "      <td>Country_01</td>\n",
       "      <td>1</td>\n",
       "      <td>Mining</td>\n",
       "      <td>I</td>\n",
       "      <td>IV</td>\n",
       "      <td>Male</td>\n",
       "      <td>Third Party</td>\n",
       "      <td>Pressed</td>\n",
       "      <td>While removing the drill rod of the Jumbo 08 f...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-01-02</td>\n",
       "      <td>Country_02</td>\n",
       "      <td>2</td>\n",
       "      <td>Mining</td>\n",
       "      <td>I</td>\n",
       "      <td>IV</td>\n",
       "      <td>Male</td>\n",
       "      <td>Employee</td>\n",
       "      <td>Pressurized Systems</td>\n",
       "      <td>During the activation of a sodium sulphide pum...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016-01-06</td>\n",
       "      <td>Country_01</td>\n",
       "      <td>3</td>\n",
       "      <td>Mining</td>\n",
       "      <td>I</td>\n",
       "      <td>III</td>\n",
       "      <td>Male</td>\n",
       "      <td>Third Party (Remote)</td>\n",
       "      <td>Manual Tools</td>\n",
       "      <td>In the sub-station MILPO located at level +170...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-01-08</td>\n",
       "      <td>Country_01</td>\n",
       "      <td>4</td>\n",
       "      <td>Mining</td>\n",
       "      <td>I</td>\n",
       "      <td>I</td>\n",
       "      <td>Male</td>\n",
       "      <td>Third Party</td>\n",
       "      <td>Others</td>\n",
       "      <td>Being 9:45 am. approximately in the Nv. 1880 C...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Friday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016-01-10</td>\n",
       "      <td>Country_01</td>\n",
       "      <td>4</td>\n",
       "      <td>Mining</td>\n",
       "      <td>IV</td>\n",
       "      <td>IV</td>\n",
       "      <td>Male</td>\n",
       "      <td>Third Party</td>\n",
       "      <td>Others</td>\n",
       "      <td>Approximately at 11:45 a.m. in circumstances t...</td>\n",
       "      <td>2016</td>\n",
       "      <td>1</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>Summer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date     Country  Local Industry Sector Accident Level  \\\n",
       "0  2016-01-01  Country_01      1          Mining              I   \n",
       "1  2016-01-02  Country_02      2          Mining              I   \n",
       "2  2016-01-06  Country_01      3          Mining              I   \n",
       "3  2016-01-08  Country_01      4          Mining              I   \n",
       "4  2016-01-10  Country_01      4          Mining             IV   \n",
       "\n",
       "  Potential Accident Level Gender      Natureofemployee        Critical Risk  \\\n",
       "0                       IV   Male           Third Party              Pressed   \n",
       "1                       IV   Male              Employee  Pressurized Systems   \n",
       "2                      III   Male  Third Party (Remote)         Manual Tools   \n",
       "3                        I   Male           Third Party               Others   \n",
       "4                       IV   Male           Third Party               Others   \n",
       "\n",
       "                                         Description  Year  Month    Weekday  \\\n",
       "0  While removing the drill rod of the Jumbo 08 f...  2016      1     Friday   \n",
       "1  During the activation of a sodium sulphide pum...  2016      1   Saturday   \n",
       "2  In the sub-station MILPO located at level +170...  2016      1  Wednesday   \n",
       "3  Being 9:45 am. approximately in the Nv. 1880 C...  2016      1     Friday   \n",
       "4  Approximately at 11:45 a.m. in circumstances t...  2016      1     Sunday   \n",
       "\n",
       "   Season  \n",
       "0  Summer  \n",
       "1  Summer  \n",
       "2  Summer  \n",
       "3  Summer  \n",
       "4  Summer  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset= pd.read_csv('data.csv')\n",
    "print(\"Shape of the dataset is :\",dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a list of possible features allowed for showing the data insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_features=['Country', 'Local', 'Industry Sector', 'Accident Level',\n",
    "       'Potential Accident Level', 'Gender', 'Natureofemployee','Year']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to be called by the Chatbot for showing data insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def show_eda(response_result):\n",
    "    eda_flag = True\n",
    "    result = list(response_result.split(\",\"))\n",
    "    result_items = len(result)\n",
    "    \n",
    "    for item in result:\n",
    "        if item not in dataset_features:\n",
    "            eda_flag = False\n",
    "    \n",
    "    if eda_flag == True:\n",
    "        if result_items == 1:\n",
    "            univariate_analysis_categorical(dataset,result[0])\n",
    "        elif result_items == 2:\n",
    "           \n",
    "            bivariate_analysis_categorical(dataset,result[0],result[1])\n",
    "        else:\n",
    "            print(\"Sorry, not able to find appropriate answer. Please ask teh question again\")\n",
    "    else:\n",
    "        print(response_result)  \n",
    "        \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def univariate_analysis_categorical(dataset,feature):    \n",
    "    print(\"\\n\")\n",
    "    print(\"===========================================================================================\")\n",
    "    print(\"Data Analysis of feature: \",feature)\n",
    "    print(\"===========================================================================================\\n\")\n",
    "                \n",
    "    print(\"\\n\")  \n",
    "    print(\"-----------------\")\n",
    "    print(\"Countplot  for feature: \",feature)\n",
    "    print(\"-----------------\")\n",
    "        \n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.countplot(dataset[feature],order = dataset[feature].value_counts().index)\n",
    "    plt.xticks(rotation = 'vertical')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"-----------------\")\n",
    "    print(\"Pie Chart for feature: \",feature)\n",
    "    print(\"------------------\")      \n",
    "        \n",
    "    labels=dataset[feature].unique()\n",
    "    plt.figure(figsize=(10,6))\n",
    "    dataset[feature].value_counts().plot.pie(autopct=\"%.1f%%\")\n",
    "    plt.show()\n",
    "            \n",
    "    \n",
    "    print(\"\\n\")\n",
    "    print(\"-----------------\")\n",
    "    print(\"Value Counts for feature: \",feature)\n",
    "    print(\"-------------------\")\n",
    "    \n",
    "    print(dataset[feature].value_counts().sort_values(ascending=False))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bivariate_analysis_categorical(dataset,feature1,feature2):\n",
    "    \n",
    "       \n",
    "            if feature1 != feature2:              \n",
    "                   \n",
    "\n",
    "                print(\"\\n\")\n",
    "                print(\"===========================================================================================\")\n",
    "                print(\"Data Analysis of features: \",feature1,' and  ', feature2)\n",
    "                print(\"===========================================================================================\")\n",
    "\n",
    "                \n",
    "                bivariate_analysis_df = pd.crosstab(index=dataset[feature1], \n",
    "                                          columns=dataset[feature2])\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(\"------------------------------------------\")\n",
    "                print(\"Cross table Analysis of features: \",feature1,' and  ', feature2)\n",
    "                print(\"------------------------------------------\")\n",
    "                \n",
    "                display(bivariate_analysis_df)\n",
    "                \n",
    "                print(\"\\n\")\n",
    "                print(\"------------------------------------------\")\n",
    "                print(\"Count plot Analysis of features: \",feature1,' and  ', feature2)\n",
    "                print(\"------------------------------------------\")\n",
    "                \n",
    "                plt.figure(figsize=(12,6))\n",
    "                sns.countplot(x=feature1, hue=feature2, data=dataset)\n",
    "                plt.show()\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling and Functions for Historical Data Insights through chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 12999  | total loss: \u001b[1m\u001b[32m0.12146\u001b[0m\u001b[0m | time: 0.056s\n",
      "| Adam | epoch: 1000 | loss: 0.12146 - acc: 0.9921 -- iter: 096/101\n",
      "Training Step: 13000  | total loss: \u001b[1m\u001b[32m0.11350\u001b[0m\u001b[0m | time: 0.064s\n",
      "| Adam | epoch: 1000 | loss: 0.11350 - acc: 0.9929 -- iter: 101/101\n",
      "--\n",
      "INFO:tensorflow:C:\\Users\\Dell\\Desktop\\GreatLearning PGP AIML\\Capstone Project\\Milestone 2\\Apoorv Working\\model_eda.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('GLSafexInfo_eda.json') as json_data:\n",
    "    intents_eda = json.load(json_data)\n",
    "\n",
    "# Initialize data fields for our file\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "# loop through each sentence in our intents patterns\n",
    "for intent in intents_eda['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add to our words list\n",
    "        words.extend(w)\n",
    "        # add to documents in our corpus\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# stem and lower each word and remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicates\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n",
    "\n",
    "\n",
    "# create our training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for our output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize our bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stem each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is a '0' for each tag and '1' for current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffle our features and turn into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# create train and test lists\n",
    "X_train = list(training[:,0])\n",
    "y_train = list(training[:,1])\n",
    "\n",
    "\n",
    "# reset underlying graph data\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "# Build neural network\n",
    "net_eda = tflearn.input_data(shape=[None, len(X_train[0])])\n",
    "net_eda = tflearn.fully_connected(net_eda, 16)\n",
    "net_eda = tflearn.fully_connected(net_eda, 16)\n",
    "net_eda = tflearn.fully_connected(net_eda, len(y_train[0]), activation='softmax')\n",
    "net_eda = tflearn.regression(net_eda)\n",
    "\n",
    "# Define model and setup tensorboard\n",
    "model_eda = tflearn.DNN(net_eda, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "# Start training (apply gradient descent algorithm)\n",
    "model_eda.fit(X_train, y_train, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model_eda.save('model_eda.tflearn')\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'X_train':X_train, 'y_train':y_train}, open( \"eda_training_data\", \"wb\" ) )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# restore all of our data structures\n",
    "import pickle\n",
    "data_eda = pickle.load( open( \"eda_training_data\", \"rb\" ) )\n",
    "words_eda = data_eda['words']\n",
    "classes_eda = data_eda['classes']\n",
    "X_train_eda = data_eda['X_train']\n",
    "y_train_eda = data_eda['y_train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\Dell\\Desktop\\GreatLearning PGP AIML\\Capstone Project\\Milestone 2\\Apoorv Working\\model_eda.tflearn\n"
     ]
    }
   ],
   "source": [
    "# import our chat-bot intents file\n",
    "import json\n",
    "with open('GLSafexInfo_eda.json') as json_data:\n",
    "    intents_eda = json.load(json_data)\n",
    "\t\n",
    "\t\n",
    "# load our saved model\n",
    "model_eda.load('./model_eda.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# functions to classify the sentences and for respective responses\n",
    "# create a data structure to hold user context\n",
    "\n",
    "context = {}\n",
    "\n",
    "ERROR_THRESHOLD = 0.25\n",
    "def classify_eda(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model_eda.predict([bow(sentence, words_eda)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes_eda[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "\n",
    "def response_eda(sentence, show_details=False):\n",
    "    \n",
    "    results = classify_eda(sentence)\n",
    "    \n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents_eda['intents']:\n",
    "                \n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    \n",
    "                    #return print(random.choice(i['responses']))\n",
    "                    response_result = random.choice(i['responses'])\n",
    "                    show_eda(response_result)\n",
    "                    \n",
    "\n",
    "            results.pop(0)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(response_eda('analysis for Accident'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Chatbot : Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clf = dataset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "#from wordcloud import WordCloud, STOPWORDS\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "      corpus=[]\n",
    "      #stem=PorterStemmer()\n",
    "      lem=WordNetLemmatizer()\n",
    "      for news in text:\n",
    "          words=[w for w in word_tokenize(news) if (w not in stop)]\n",
    "          \n",
    "          words=[lem.lemmatize(w) for w in words if len(w)>2]\n",
    "          words = [''.join(c for c in s if c not in string.punctuation) for s in words if s]\n",
    "          words = [word.lower() for word in words]\n",
    "          words = [word for word in words if word.isalpha()]\n",
    "          corpus.append(words) \n",
    "         \n",
    "      return corpus      \n",
    "      \n",
    "dataset_clf['processed_text']= preprocess_text(dataset_clf['Description'])\n",
    "\n",
    "desc_processed = []\n",
    "for i in range(len(dataset_clf['processed_text'])):\n",
    "   desc_processed.append(' '.join(wrd for wrd in dataset_clf.iloc[:,14][i]))\n",
    "\n",
    "dataset_clf['description_processed'] = desc_processed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and functions for  Accident Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = dataset_clf['description_processed']\n",
    "y = dataset_clf['Accident Level']\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "Xc = vectorizer.fit_transform(X).toarray()\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc = LinearSVC( max_iter=2500)\n",
    "svc.fit(Xc_train, yc_train)\n",
    "yc_pred_SVC = svc.predict(Xc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the SVC model : 99.44\n",
      "Test accuracy of the SVC model : 79.37\n"
     ]
    }
   ],
   "source": [
    "acc_svc = accuracy_score(yc_test, yc_pred_SVC)\n",
    "acc_svc_tr = svc.score(Xc_train, yc_train)\n",
    "print(\"Train accuracy of the SVC model : {:.2f}\".format(acc_svc_tr*100))\n",
    "print(\"Test accuracy of the SVC model : {:.2f}\".format(acc_svc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           I       0.80      1.00      0.89        48\n",
      "          II       0.00      0.00      0.00         5\n",
      "         III       1.00      0.25      0.40         4\n",
      "          IV       1.00      0.20      0.33         5\n",
      "           V       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.79        63\n",
      "   macro avg       0.56      0.29      0.32        63\n",
      "weighted avg       0.75      0.79      0.73        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:',classification_report(yc_test, yc_pred_SVC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'svc_model.sav'\n",
    "pickle.dump(svc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_acclevel = pickle.load(open('./svc_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_acclevel(sentence):\n",
    "    \n",
    "    df_user_input=pd.DataFrame()\n",
    "    df=pd.DataFrame()\n",
    "    user_input = {'Description': sentence}\n",
    "    df_user_input = df_user_input.append(user_input, ignore_index = True)\n",
    "    df_user_input['processed_text']= preprocess_text(df_user_input['Description'])\n",
    "    df['processed_text']=df_user_input['processed_text']\n",
    "    df.reset_index(inplace=True)\n",
    "    desc_processed = []\n",
    "    for i in range(len(df['processed_text'])):\n",
    "        desc_processed.append(' '.join(wrd for wrd in df.iloc[:,1][i]))\n",
    "    test = vectorizer.transform(desc_processed)\n",
    "    result = loaded_model_acclevel.predict(test)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling and functions for Potential Accident Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count vectorization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "X = dataset_clf['description_processed']\n",
    "y = dataset_clf['Potential Accident Level']\n",
    "\n",
    "vectorizer = CountVectorizer(binary=True, ngram_range=(1, 2))\n",
    "Xc = vectorizer.fit_transform(X).toarray()\n",
    "Xc_train, Xc_test, yc_train, yc_test = train_test_split(Xc, y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc_potacclevel = LinearSVC( max_iter=2500)\n",
    "svc_potacclevel.fit(Xc_train, yc_train)\n",
    "yc_pred_SVC_potacclevel = svc_potacclevel.predict(Xc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy of the SVC model : 19.44\n",
      "Test accuracy of the SVC model : 46.03\n"
     ]
    }
   ],
   "source": [
    "acc_svc_potacclevel = accuracy_score(yc_test, yc_pred_SVC_potacclevel)\n",
    "acc_svc_tr_potacclevel = svc.score(Xc_train, yc_train)\n",
    "print(\"Train accuracy of the SVC model : {:.2f}\".format(acc_svc_tr_potacclevel*100))\n",
    "print(\"Test accuracy of the SVC model : {:.2f}\".format(acc_svc_potacclevel*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:               precision    recall  f1-score   support\n",
      "\n",
      "           I       1.00      0.14      0.25         7\n",
      "          II       0.43      0.55      0.48        11\n",
      "         III       0.35      0.32      0.33        19\n",
      "          IV       0.52      0.70      0.59        23\n",
      "           V       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.46        63\n",
      "   macro avg       0.46      0.34      0.33        63\n",
      "weighted avg       0.48      0.46      0.43        63\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Classification report:',classification_report(yc_test, yc_pred_SVC_potacclevel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "filename = 'svc_potacclevel_model.sav'\n",
    "pickle.dump(svc_potacclevel, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model_potacclevel = pickle.load(open('./svc_potacclevel_model.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_potacclevel(sentence):\n",
    "    \n",
    "    df_user_input=pd.DataFrame()\n",
    "    df=pd.DataFrame()\n",
    "    user_input = {'Description': sentence}\n",
    "    df_user_input = df_user_input.append(user_input, ignore_index = True)\n",
    "    df_user_input['processed_text']= preprocess_text(df_user_input['Description'])\n",
    "    df['processed_text']=df_user_input['processed_text']\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    desc_processed = []\n",
    "    for i in range(len(df['processed_text'])):\n",
    "        desc_processed.append(' '.join(wrd for wrd in df.iloc[:,1][i]))\n",
    "    test = vectorizer.transform(desc_processed)\n",
    "    result = loaded_model_potacclevel.predict(test)\n",
    "    return result\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with Conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot function for Question and Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chatbot_qna():\n",
    "    flag=True\n",
    "    print('\\n')\n",
    "    print(\"BOT: I am your Virtual assistant. I will try to answer your questions on Accidents Data! \\\n",
    "    \\n \\t If you want to exit any time, just type Bye!\")\n",
    "    while(flag==True):\n",
    "        print('\\n')\n",
    "        user_response = input(\"You: \")\n",
    "        user_response=user_response.lower()\n",
    "        if(user_response!='bye') and (user_response!='exit') and (user_response!='quit'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print('\\n')\n",
    "                print(\"BOT: You are welcome..\")\n",
    "                print(\"BOT: Exited from Question and Answer Module\")\n",
    "\n",
    "            else:\n",
    "                print('\\n')\n",
    "                response(user_response)\n",
    "                print('\\n')\n",
    "        else:\n",
    "            flag=False\n",
    "            print('\\n')\n",
    "            print(\"BOT: Exited from Question and Answer Module\")\n",
    "            #print(\"BOT: Goodbye! Take care  \")\n",
    "            print('\\n')\n",
    "            print('-----------------------------------------------------')\n",
    "            print('\\n')\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot function for Accident Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_acclevel():\n",
    "    flag=True\n",
    "    print('\\n')\n",
    "    print(\"BOT: I am your Virtual assistant. I will try to Predict Accident Level for your description \\\n",
    "    \\n \\t If you want to exit any time, just type Bye!\")\n",
    "    while(flag==True):\n",
    "        print('\\n')\n",
    "        user_response = input(\"You: \")\n",
    "        user_response=user_response.lower()\n",
    "        if(user_response!='bye') and (user_response!='exit') and (user_response!='quit'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print('\\n')\n",
    "                print(\"BOT: You are welcome..\")\n",
    "                print(\"BOT: Exited from Accident Level Predictor\")\n",
    "\n",
    "            else:\n",
    "                print('\\n')\n",
    "                response = response_acclevel(user_response)\n",
    "                \n",
    "                if response:\n",
    "                    print (\"Bot: Accident Level is --> \",response[0])\n",
    "                print('\\n')\n",
    "        else:\n",
    "            flag=False\n",
    "            print('\\n')\n",
    "            print(\"BOT: Exited from Accident Level Predictor Module\")\n",
    "            #print(\"BOT: Goodbye! Take care  \")\n",
    "            print('\\n')\n",
    "            print('-----------------------------------------------------')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chatbot function for Potential Accident Level Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_potacclevel():\n",
    "    flag=True\n",
    "    print('\\n')\n",
    "    print(\"BOT: I am your Virtual assistant. I will try to predict Potential Accident Level for your description \\\n",
    "    \\n \\t If you want to exit any time, just type Bye!\")\n",
    "    while(flag==True):\n",
    "        print('\\n')\n",
    "        user_response = input(\"You: \")\n",
    "        user_response=user_response.lower()\n",
    "        if(user_response!='bye') and (user_response!='exit') and (user_response!='quit'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print('\\n')\n",
    "                print(\"BOT: You are welcome..\")\n",
    "                print(\"BOT: Exited from Potential Accident Level Predictor\")\n",
    "\n",
    "            else:\n",
    "                print('\\n')\n",
    "                response = response_potacclevel(user_response)\n",
    "                \n",
    "                if response:\n",
    "                    print (\"Bot: Potential Accident Level is --> \",response[0])\n",
    "                print('\\n')\n",
    "        else:\n",
    "            flag=False\n",
    "            print('\\n')\n",
    "            print(\"BOT: Exited from Potential Accident Level Predictor Module\")\n",
    "            #print(\"BOT: Goodbye! Take care  \")\n",
    "            print('\\n')\n",
    "            print('-----------------------------------------------------')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat bot function for showing Historical Data Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def chatbot_eda():\n",
    "    flag=True\n",
    "    print('\\n')\n",
    "    print(\"BOT: I am your Virtual assistant. I will help you with Insights of Accident Historical Data! \\\n",
    "          \\n \\t If you want to exit any time, just type Bye!\")\n",
    "    while(flag==True):\n",
    "        \n",
    "        print('\\n')\n",
    "        user_response = input(\"You: \")\n",
    "        user_response=user_response.lower()\n",
    "        if(user_response!='bye') and (user_response!='exit') and (user_response!='quit'):\n",
    "            if(user_response=='thanks' or user_response=='thank you' ):\n",
    "                flag=False\n",
    "                print('\\n')\n",
    "                print(\"BOT: You are welcome..\")\n",
    "                print(\"BOT: Exited from Data Insights Module\")\n",
    "\n",
    "            else:                    \n",
    "                    print('\\n')\n",
    "                    response_eda(user_response)\n",
    "                    print('\\n')\n",
    "        else:\n",
    "            flag=False\n",
    "            print('\\n')\n",
    "            #print(\"BOT: Goodbye! Take care  \")\n",
    "            print(\"BOT: Exited from Data Insights Module\")\n",
    "            print('\\n')\n",
    "            print('-----------------------------------------------------')\n",
    "            print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot : Home Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BOT: Please Select appropriate option:           \n",
      " \t            \n",
      " \t 1 --> For Question and Answers           \n",
      " \t 2 --> For finding the Accident level for your problem           \n",
      " \t 3 --> For finding the Potential Accident level for your problem           \n",
      " \t 4 --> For Historical Data Insights           \n",
      " \t            \n",
      " \t if you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: 1\n",
      "\n",
      "\n",
      "BOT: I am your Virtual assistant. I will try to answer your questions on Accidents Data!     \n",
      " \t If you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: tell me details on issue realted to zinc\n",
      "\n",
      "\n",
      "BOT:  Based on the historical facts it has been observed  that Employees in the Metal Industry have met with accidents related to volumetric balloon, cyclone duct, conveyor, Zinc which have result in small burns on the body parts including face as well. So it is always recommended to take proper precautions and use face and body covers while working on such areas\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You: quit\n",
      "\n",
      "\n",
      "BOT: Exited from Question and Answer Module\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "BOT: Please Select appropriate option:           \n",
      " \t            \n",
      " \t 1 --> For Question and Answers           \n",
      " \t 2 --> For finding the Accident level for your problem           \n",
      " \t 3 --> For finding the Potential Accident level for your problem           \n",
      " \t 4 --> For Historical Data Insights           \n",
      " \t            \n",
      " \t if you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: 2\n",
      "\n",
      "\n",
      "BOT: I am your Virtual assistant. I will try to Predict Accident Level for your description     \n",
      " \t If you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: When the plant operator was semi-kneeling when lifting the lid or gate (15 kg) of the distributor box of the secondary mill No. 4 and No. 5, his right knee slips due to the presence of debris spilled on the platform or floor (Grating) - which gave him an extra effort in his left leg, generating a muscle contracture.\n",
      "\n",
      "\n",
      "Bot: Accident Level is -->  I\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You: bye\n",
      "\n",
      "\n",
      "BOT: Exited from Accident Level Predictor Module\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "BOT: Please Select appropriate option:           \n",
      " \t            \n",
      " \t 1 --> For Question and Answers           \n",
      " \t 2 --> For finding the Accident level for your problem           \n",
      " \t 3 --> For finding the Potential Accident level for your problem           \n",
      " \t 4 --> For Historical Data Insights           \n",
      " \t            \n",
      " \t if you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: 3\n",
      "\n",
      "\n",
      "BOT: I am your Virtual assistant. I will try to predict Potential Accident Level for your description     \n",
      " \t If you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: When the plant operator was semi-kneeling when lifting the lid or gate (15 kg) of the distributor box of the secondary mill No. 4 and No. 5, his right knee slips due to the presence of debris spilled on the platform or floor (Grating) - which gave him an extra effort in his left leg, generating a muscle contracture.\n",
      "\n",
      "\n",
      "Bot: Potential Accident Level is -->  III\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "You: bye\n",
      "\n",
      "\n",
      "BOT: Exited from Potential Accident Level Predictor Module\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "BOT: Please Select appropriate option:           \n",
      " \t            \n",
      " \t 1 --> For Question and Answers           \n",
      " \t 2 --> For finding the Accident level for your problem           \n",
      " \t 3 --> For finding the Potential Accident level for your problem           \n",
      " \t 4 --> For Historical Data Insights           \n",
      " \t            \n",
      " \t if you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: give me details about country and industry\n",
      "\n",
      "BOT: Please Select appropriate option:           \n",
      " \t            \n",
      " \t 1 --> For Question and Answers           \n",
      " \t 2 --> For finding the Accident level for your problem           \n",
      " \t 3 --> For finding the Potential Accident level for your problem           \n",
      " \t 4 --> For Historical Data Insights           \n",
      " \t            \n",
      " \t if you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: 4\n",
      "\n",
      "\n",
      "BOT: I am your Virtual assistant. I will help you with Insights of Accident Historical Data!           \n",
      " \t If you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: You: give me details about country and industry\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===========================================================================================\n",
      "Data Analysis of features:  Country  and   Industry Sector\n",
      "===========================================================================================\n",
      "\n",
      "\n",
      "------------------------------------------\n",
      "Cross table Analysis of features:  Country  and   Industry Sector\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Industry Sector</th>\n",
       "      <th>Metals</th>\n",
       "      <th>Mining</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Country_01</th>\n",
       "      <td>46</td>\n",
       "      <td>200</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country_02</th>\n",
       "      <td>88</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Country_03</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Industry Sector  Metals  Mining  Others\n",
       "Country                                \n",
       "Country_01           46     200       2\n",
       "Country_02           88      37       4\n",
       "Country_03            0       0      41"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------------------------------\n",
      "Count plot Analysis of features:  Country  and   Industry Sector\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "You: exit\n",
      "\n",
      "\n",
      "BOT: Exited from Data Insights Module\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "\n",
      "\n",
      "BOT: Please Select appropriate option:           \n",
      " \t            \n",
      " \t 1 --> For Question and Answers           \n",
      " \t 2 --> For finding the Accident level for your problem           \n",
      " \t 3 --> For finding the Potential Accident level for your problem           \n",
      " \t 4 --> For Historical Data Insights           \n",
      " \t            \n",
      " \t if you want to exit any time, just type Bye!\n",
      "\n",
      "\n",
      "You: exit\n",
      "\n",
      "\n",
      "BOT: Goodbye! Take care  \n",
      "\n",
      "\n",
      "================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mainflag=True\n",
    "flag = False\n",
    "\n",
    "while(mainflag==True):\n",
    "    if flag == False:\n",
    "        print('')\n",
    "        print(\"BOT: Please Select appropriate option: \\\n",
    "          \\n \\t  \\\n",
    "          \\n \\t 1 --> For Question and Answers \\\n",
    "          \\n \\t 2 --> For finding the Accident level for your problem \\\n",
    "          \\n \\t 3 --> For finding the Potential Accident level for your problem \\\n",
    "          \\n \\t 4 --> For Historical Data Insights \\\n",
    "          \\n \\t  \\\n",
    "          \\n \\t if you want to exit any time, just type Bye!\")\n",
    "        \n",
    "    print('\\n')\n",
    "    user_response = input(\"You: \")\n",
    "    user_response=user_response.lower()\n",
    "    if(user_response!='bye') and (user_response!='exit') and (user_response!='quit'):\n",
    "        if(user_response=='thanks' or user_response=='thank you' ):\n",
    "            mainflag=False\n",
    "            print('\\n')\n",
    "            print(\"BOT: You are welcome..\")\n",
    "        \n",
    "        elif (user_response=='1'):\n",
    "                chatbot_qna()\n",
    "        elif (user_response=='2'):\n",
    "                chatbot_acclevel()\n",
    "        elif (user_response=='3'):\n",
    "                chatbot_potacclevel()   \n",
    "        elif (user_response=='4'):\n",
    "                chatbot_eda()  \n",
    "                \n",
    "    else:\n",
    "        mainflag=False\n",
    "        print('\\n')\n",
    "        print(\"BOT: Goodbye! Take care  \")\n",
    "        print('\\n')\n",
    "        print('================================================================')\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
